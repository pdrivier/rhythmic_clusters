{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fd6a79",
   "metadata": {},
   "source": [
    "# fit kde models over the course of odor sampling\n",
    "To figure out how likely it is for multiple rhythmic relationships to _**coexist**_ in time, or to appear orthogonally as an interneuron shifts its spike timing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17558e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import collections \n",
    "import kde_spikephase as kd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from kde_spikephase import kde_spikephase_estimator_forarray\n",
    "from mdl_eval_tools import bayes,kl,logloss\n",
    "from pprint import pprint\n",
    "from process_lfps import create_data_segments\n",
    "from rayleigh_pr import rayleigh_pr\n",
    "from scipy.signal import hilbert\n",
    "from scipy import stats\n",
    "from scipy.stats import sem\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.neighbors import KernelDensity\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sns.set(font_scale=1.5,style='whitegrid')\n",
    "\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffeaaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neuron_cols(df,neuron_expression):\n",
    "    \"\"\"df: dataframe containing columns corresponding to neuron spiketimes\"\"\"\n",
    "    \n",
    "    pattern = re.compile(neuron_expression)\n",
    "    neuron_cols = []\n",
    "    for s in df.columns:\n",
    "\n",
    "        matched = pattern.match(s)\n",
    "        is_match = bool(matched)\n",
    "\n",
    "        if is_match:\n",
    "\n",
    "            neuron_cols.append(matched.string)\n",
    "    \n",
    "    return neuron_cols\n",
    "\n",
    "def get_allowable_lfp_wires(neuron_id,split_on):\n",
    "    \"\"\"neuron_id: string, e.g. TETSPK53b\n",
    "    split_on: string, e.g. 'K', you will use this to exclude the unit letter from neuron_id\"\"\"\n",
    "\n",
    "    #figure out who is the corresponding LFP for this neuron, must be on same tetrode!\n",
    "    unit_wirenum = neuron.split(split_on)[1]\n",
    "    unit_wirenum = unit_wirenum[:-1]\n",
    "    if len(unit_wirenum) > 2:\n",
    "        unit_wirenum = unit_wirenum[:2]\n",
    "    allow_wire = [int(unit_wirenum), int(unit_wirenum) + 1, int(unit_wirenum) + 2, int(unit_wirenum) + 3]\n",
    "    allow_wire = [str(w) for w in allow_wire]\n",
    "    allow_wire = ['0'+w if int(w)<10 else w for w in allow_wire]\n",
    "    \n",
    "    return allow_wire\n",
    "\n",
    "def get_lfp_cols(df,lfp_expression):\n",
    "    \"\"\"df: dataframe containing columns corresponding to neuron spiketimes\n",
    "    lfp_expression: string, e.g.\"\"\"\n",
    "#example lfp_expression =  \"TETFP\"+allow_wire[0]+\"\\Z|TETFP\"+allow_wire[1]+\"\\Z|TETFP\"+allow_wire[2]+\"\\Z|TETFP\"+allow_wire[3]+\"\\Z\"\n",
    "        \n",
    "    pattern = re.compile(lfp_expression)\n",
    "    lfp_cols = []\n",
    "    for s in df.columns:\n",
    "\n",
    "        matched = pattern.match(s)\n",
    "        is_match = bool(matched)\n",
    "\n",
    "        if is_match:\n",
    "\n",
    "            lfp_cols.append(matched.string)\n",
    "\n",
    "    return lfp_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685f0d1",
   "metadata": {},
   "source": [
    "### set up your fitting params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793de13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_kde = {'n_splits': 7, #7, # 7-fold uses 85% of data to train, 15% to validate\n",
    "             'grid_size': 1000,\n",
    "             'bw_narrow': 1/30,\n",
    "             'bw_wide': 1/0.6,\n",
    "             'n_bw': 20}\n",
    "\n",
    "n_splits = setup_kde.get('n_splits')\n",
    "grid_size = setup_kde.get('grid_size')\n",
    "bw_narrow = setup_kde.get('bw_narrow')\n",
    "bw_wide = setup_kde.get('bw_wide')\n",
    "n_bw = setup_kde.get('n_bw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fa31e",
   "metadata": {},
   "source": [
    "### fit models to early and late trial segments\n",
    "LH2 & LH3 need to have their early window at trial start time, because that is their odor onset\n",
    "LH8,LH9, and LH16 need to have their early window at 250ms from start time, because that is their odor onset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448ee78",
   "metadata": {},
   "source": [
    "##### LH2 & LH3\n",
    "\n",
    "Treat these guys differently because they got the odor on the nosepoke onset, rather than at 250 ms post nosepoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = 'over_time_models_ints_250mswins/'\n",
    "\n",
    "datapath = 'python_spkphase_odorsamp/'\n",
    "files = os.listdir(datapath)\n",
    "files = [f for f in files if not f.startswith('.')]\n",
    "files = [f for f in files if f.startswith('LH02') | f.startswith('LH03')]\n",
    "\n",
    "\n",
    "rhythms = ['theta', 'beta', 'lowgamma', 'highgamma']\n",
    "quarter_labels = [1,2,3,4]\n",
    "#find the columns in the df containing unique neurons' spikes\n",
    "\n",
    "window_dur = 250 #ms \n",
    "for file in tqdm(files, position=0, desc=\"file\", leave=True, colour='cornflowerblue'):\n",
    "\n",
    "    df = pd.read_csv(os.path.join(datapath,file))\n",
    "    \n",
    "    #grab just the time samples corresponding to the odor sampling period\n",
    "    odorsamp_df = df[df['trial_segment']=='dur']\n",
    "    df = [] #empty this to save memory\n",
    "    \n",
    "    neuron_cols = get_neuron_cols(odorsamp_df,'TETSPK')\n",
    "\n",
    "    kde_df = [] #initialize the df that will contain all of this file's data\n",
    "    #get the lfp phases for each rhythm\n",
    "    \n",
    "    for quarter in tqdm(quarter_labels,position=1, desc=\"rhythm\", leave=False, colour='lavender'):\n",
    "        \n",
    "        #grab just the current quarter/odor set's data\n",
    "        subq = odorsamp_df[odorsamp_df['quarter_labels'] == quarter]\n",
    "        \n",
    "        if np.all(subq['accuracy'] >= .75):\n",
    "            \n",
    "            neuron_cols = get_neuron_cols(subq,'TETSPK')\n",
    "            \n",
    "            for rhythm in tqdm(rhythms,position=2, desc=\"rhythm\", leave=False, colour='hotpink'):\n",
    "                \n",
    "                for neuron in tqdm(neuron_cols, position=3, desc=\"neuron\", leave=False, colour='darkturquoise'):\n",
    "\n",
    "                    cell_id = file.split('_')[0] + '_' + file.split('_')[1] + '_' + neuron\n",
    "\n",
    "                    allow_wire = get_allowable_lfp_wires(neuron,'K')\n",
    "\n",
    "                    #grab the lfps\n",
    "                    lfp_cols = get_lfp_cols(subq,\"TETFP\"+allow_wire[0]+\"\\Z|TETFP\"+allow_wire[1]+\"\\Z|TETFP\"+allow_wire[2]+\"\\Z|TETFP\"+allow_wire[3]+\"\\Z\")\n",
    "                    lfp_name = lfp_cols[0]\n",
    "\n",
    "                    #create the phase columns using the hilbert transform on the filtered lfp\n",
    "                    filt_name = lfp_name + 'filt_' + rhythm \n",
    "\n",
    "                    filtered = subq[filt_name]\n",
    "                    analytic_signal = hilbert(filtered)\n",
    "\n",
    "                    phase_name = lfp_name + 'phase_' + rhythm \n",
    "                    subq[phase_name] = np.arctan2(filtered,analytic_signal.imag)\n",
    "                    \n",
    "                    #get the unique odor position identifiers for each condition\n",
    "                    condition_labels = subq.groupby(['odor_labels']).sum().index\n",
    "                    \n",
    "                    for condition in condition_labels: \n",
    "                        \n",
    "                        odor = condition\n",
    "                        \n",
    "                        d = subq[subq['odor_labels']==odor]\n",
    "                        \n",
    "                        ntrials = len(set(d['trial_labels']))\n",
    "                        \n",
    "                        #trial criterion\n",
    "                        if ntrials >= 4: \n",
    "                           \n",
    "                            #set up a new column that labels the fitting windows\n",
    "                            #for each trial\n",
    "                            blocks_and_trials = d.groupby(['odor_block_labels','trial_labels']).count().index\n",
    "                            window_one = []\n",
    "                            window_two = []\n",
    "                            window_three = []\n",
    "                            window_four = []\n",
    "                            window_five = []\n",
    "                            window_six = []\n",
    "                            for block_and_trial in blocks_and_trials:\n",
    "\n",
    "                                block = block_and_trial[0]\n",
    "                                trial = block_and_trial[1]\n",
    "\n",
    "                                subtrial = d[(d['odor_block_labels']==block) & (d['trial_labels']==trial)]\n",
    "#                                 window_nofit0.append(np.repeat('nofit',250))\n",
    "                                window_one.append(np.repeat('one',window_dur))\n",
    "                                window_two.append(np.repeat('two',window_dur))\n",
    "                                window_three.append(np.repeat('three',window_dur))\n",
    "                                window_four.append(np.repeat('four',window_dur))\n",
    "                                window_five.append(np.repeat('five',window_dur))\n",
    "                                window_six.append(np.repeat('six',window_dur+1))\n",
    "\n",
    "                            all_windows = np.hstack((window_one,\n",
    "                                                     window_two,\n",
    "                                                     window_three,\n",
    "                                                     window_four,\n",
    "                                                     window_five,\n",
    "                                                     window_six\n",
    "                                                    ))\n",
    "                            flattened = [item for sublist in all_windows for item in sublist]\n",
    "\n",
    "                            d['fitting_windows'] = flattened\n",
    "\n",
    "                            fitting_windows = ['one',\n",
    "                                               'two',\n",
    "                                               'three',\n",
    "                                               'four',\n",
    "                                               'five',\n",
    "                                               'six'\n",
    "                                              ]\n",
    "\n",
    "                            for window in fitting_windows: \n",
    "\n",
    "                                subwin = d[d['fitting_windows']==window]\n",
    "                                #now, grab the spikes and phases to fit the kde curves\n",
    "                                #grab data\n",
    "                                spikes = subwin[neuron].values\n",
    "                                phases = subwin[phase_name].values\n",
    "\n",
    "                                phi_when_sp = []\n",
    "                                for s,p in zip(spikes,phases):\n",
    "                                    if s==1: \n",
    "                                        phi_when_sp.append(p)\n",
    "\n",
    "                                phi_when_sp = np.array(phi_when_sp)\n",
    "\n",
    "                                #CRITERION: number of spikes must exceed the number of cross val splits for fitting kde\n",
    "                                if n_splits < sum(spikes):\n",
    "\n",
    "                                    kde_phaseprob, kde, bw = kde_spikephase_estimator_forarray(phi_when_sp,n_splits,grid_size,bw_narrow,bw_wide,n_bw)\n",
    "\n",
    "                                    spike_prior = subwin[neuron].sum()/subwin.shape[0]\n",
    "                                    phase_prior = 1/(2*np.pi)\n",
    "                                    kde_spikeprob = bayes(kde_phaseprob,\n",
    "                                                            spike_prior,\n",
    "                                                            phase_prior\n",
    "                                      )\n",
    "\n",
    "                                elif n_splits >= sum(spikes): \n",
    "                                    \n",
    "                                    avg_pspk = sum(spikes)/len(spikes)\n",
    "                                    kde_spikeprob = np.repeat(avg_pspk,1000)\n",
    "                                \n",
    "                                dct = {'cell_id': cell_id,\n",
    "                                       'quarter_labels': np.repeat(quarter,len(kde_spikeprob)),\n",
    "                                       'rhythm': np.repeat(rhythm,len(kde_spikeprob)),\n",
    "                                       'kde_spikeprob': kde_spikeprob,\n",
    "                                       'kde_phaseprob': kde_phaseprob,\n",
    "                                       'condition_labels': np.repeat(odor,len(kde_spikeprob)),\n",
    "                                       'fitting_window': window\n",
    "                                      }\n",
    "                                kde_df.append(pd.DataFrame(dct))\n",
    "        \n",
    "                \n",
    "    kde_odor = pd.concat(kde_df)               \n",
    "    if not os.path.exists(savepath): \n",
    "        os.makedirs(savepath)\n",
    "\n",
    "    savename = file.split('_')[0] + '_' + file.split('_')[1] + '_kde_over_time.csv'\n",
    "    kde_odor.to_csv(os.path.join(savepath,savename))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c72d15b",
   "metadata": {},
   "source": [
    "##### LH8,LH9,LH16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d356818",
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = 'over_time_models_ints_250mswins/'\n",
    "\n",
    "datapath = 'python_spkphase_odorsamp/'\n",
    "files = os.listdir(datapath)\n",
    "files = [f for f in files if not f.startswith('.')]\n",
    "files = [f for f in files if f.startswith('LH08') | f.startswith('LH09') | f.startswith('LH16')]\n",
    "\n",
    "\n",
    "rhythms = ['theta', 'beta', 'lowgamma', 'highgamma']\n",
    "quarter_labels = [1,2,3,4]\n",
    "#find the columns in the df containing unique neurons' spikes\n",
    "\n",
    "window_dur = 250 #ms \n",
    "for file in tqdm(files, position=0, desc=\"file\", leave=True, colour='cornflowerblue'):\n",
    "\n",
    "    df = pd.read_csv(os.path.join(datapath,file))\n",
    "    \n",
    "    #grab just the time samples corresponding to the odor sampling period\n",
    "    odorsamp_df = df[df['trial_segment']=='dur']\n",
    "    df = [] #empty this to save memory\n",
    "    \n",
    "    neuron_cols = get_neuron_cols(odorsamp_df,'TETSPK')\n",
    "\n",
    "    kde_df = [] #initialize the df that will contain all of this file's data\n",
    "    #get the lfp phases for each rhythm\n",
    "    \n",
    "    for quarter in tqdm(quarter_labels,position=1, desc=\"rhythm\", leave=False, colour='lavender'):\n",
    "        \n",
    "        #grab just the current quarter/odor set's data\n",
    "        subq = odorsamp_df[odorsamp_df['quarter_labels'] == quarter]\n",
    "        \n",
    "        if np.all(subq['accuracy'] >= .75):\n",
    "            \n",
    "            neuron_cols = get_neuron_cols(subq,'TETSPK')\n",
    "            \n",
    "            for rhythm in tqdm(rhythms,position=2, desc=\"rhythm\", leave=False, colour='hotpink'):\n",
    "                \n",
    "                for neuron in tqdm(neuron_cols, position=3, desc=\"neuron\", leave=False, colour='darkturquoise'):\n",
    "\n",
    "                    cell_id = file.split('_')[0] + '_' + file.split('_')[1] + '_' + neuron\n",
    "\n",
    "                    allow_wire = get_allowable_lfp_wires(neuron,'K')\n",
    "\n",
    "                    #grab the lfps\n",
    "                    lfp_cols = get_lfp_cols(subq,\"TETFP\"+allow_wire[0]+\"\\Z|TETFP\"+allow_wire[1]+\"\\Z|TETFP\"+allow_wire[2]+\"\\Z|TETFP\"+allow_wire[3]+\"\\Z\")\n",
    "                    lfp_name = lfp_cols[0]\n",
    "\n",
    "                    #create the phase columns using the hilbert transform on the filtered lfp\n",
    "                    filt_name = lfp_name + 'filt_' + rhythm \n",
    "\n",
    "                    filtered = subq[filt_name]\n",
    "                    analytic_signal = hilbert(filtered)\n",
    "\n",
    "                    phase_name = lfp_name + 'phase_' + rhythm \n",
    "                    subq[phase_name] = np.arctan2(filtered,analytic_signal.imag)\n",
    "                    \n",
    "                    #get the unique odor position identifiers for each condition\n",
    "                    condition_labels = subq.groupby(['odor_labels']).sum().index\n",
    "                    \n",
    "                    for condition in condition_labels: \n",
    "                        \n",
    "                        odor = condition\n",
    "                        \n",
    "                        d = subq[subq['odor_labels']==odor]\n",
    "                        \n",
    "                        ntrials = len(set(d['trial_labels']))\n",
    "                        \n",
    "                        #trial criterion\n",
    "                        if ntrials >= 4: \n",
    "                           \n",
    "                            #set up a new column that labels the fitting windows\n",
    "                            #for each trial\n",
    "                            blocks_and_trials = d.groupby(['odor_block_labels','trial_labels']).count().index\n",
    "                            window_nofit = []\n",
    "                            window_one = []\n",
    "                            window_two = []\n",
    "                            window_three = []\n",
    "                            window_four = []\n",
    "                            window_five = []\n",
    "                            for block_and_trial in blocks_and_trials:\n",
    "\n",
    "                                block = block_and_trial[0]\n",
    "                                trial = block_and_trial[1]\n",
    "\n",
    "                                subtrial = d[(d['odor_block_labels']==block) & (d['trial_labels']==trial)]\n",
    "#                                 window_nofit0.append(np.repeat('nofit',250))\n",
    "                                window_nofit.append(np.repeat('nofit',window_dur))\n",
    "                                window_one.append(np.repeat('one',window_dur))\n",
    "                                window_two.append(np.repeat('two',window_dur))\n",
    "                                window_three.append(np.repeat('three',window_dur))\n",
    "                                window_four.append(np.repeat('four',window_dur))\n",
    "                                window_five.append(np.repeat('five',window_dur+1))\n",
    "\n",
    "                            all_windows = np.hstack((window_nofit,\n",
    "                                                     window_one,\n",
    "                                                     window_two,\n",
    "                                                     window_three,\n",
    "                                                     window_four,\n",
    "                                                     window_five\n",
    "                                                    ))\n",
    "                            flattened = [item for sublist in all_windows for item in sublist]\n",
    "\n",
    "                            d['fitting_windows'] = flattened\n",
    "\n",
    "                            fitting_windows = ['nofit',\n",
    "                                               'one',\n",
    "                                               'two',\n",
    "                                               'three',\n",
    "                                               'four',\n",
    "                                               'five'\n",
    "                                              ]\n",
    "\n",
    "                            for window in fitting_windows: \n",
    "\n",
    "                                subwin = d[d['fitting_windows']==window]\n",
    "                                #now, grab the spikes and phases to fit the kde curves\n",
    "                                #grab data\n",
    "                                spikes = subwin[neuron].values\n",
    "                                phases = subwin[phase_name].values\n",
    "\n",
    "                                phi_when_sp = []\n",
    "                                for s,p in zip(spikes,phases):\n",
    "                                    if s==1: \n",
    "                                        phi_when_sp.append(p)\n",
    "\n",
    "                                phi_when_sp = np.array(phi_when_sp)\n",
    "\n",
    "                                #CRITERION: number of spikes must exceed the number of cross val splits for fitting kde\n",
    "                                if n_splits < sum(spikes):\n",
    "\n",
    "                                    kde_phaseprob, kde, bw = kde_spikephase_estimator_forarray(phi_when_sp,n_splits,grid_size,bw_narrow,bw_wide,n_bw)\n",
    "\n",
    "                                    spike_prior = subwin[neuron].sum()/subwin.shape[0]\n",
    "                                    phase_prior = 1/(2*np.pi)\n",
    "                                    kde_spikeprob = bayes(kde_phaseprob,\n",
    "                                                            spike_prior,\n",
    "                                                            phase_prior\n",
    "                                      )\n",
    "\n",
    "                                elif n_splits >= sum(spikes): \n",
    "                                    \n",
    "                                    avg_pspk = sum(spikes)/len(spikes)\n",
    "                                    kde_spikeprob = np.repeat(avg_pspk,1000)\n",
    "                                    \n",
    "                                dct = {'cell_id': cell_id,\n",
    "                                       'quarter_labels': np.repeat(quarter,len(kde_spikeprob)),\n",
    "                                       'rhythm': np.repeat(rhythm,len(kde_spikeprob)),\n",
    "                                       'kde_spikeprob': kde_spikeprob,\n",
    "                                       'kde_phaseprob': kde_phaseprob,\n",
    "                                       'condition_labels': np.repeat(odor,len(kde_spikeprob)),\n",
    "                                       'fitting_window': window\n",
    "                                      }\n",
    "                                kde_df.append(pd.DataFrame(dct))\n",
    "        \n",
    "                \n",
    "    kde_odor = pd.concat(kde_df)               \n",
    "    if not os.path.exists(savepath): \n",
    "        os.makedirs(savepath)\n",
    "\n",
    "    savename = file.split('_')[0] + '_' + file.split('_')[1] + '_kde_over_time.csv'\n",
    "    kde_odor.to_csv(os.path.join(savepath,savename))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5798ea",
   "metadata": {},
   "source": [
    "## now, get the sumklds for each phase model, for each window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 7\n",
    "rhythms = ['theta','lowgamma','highgamma']\n",
    "\n",
    "datapath = 'over_time_modelsODOR_ints_250mswins/'\n",
    "files = os.listdir(datapath)\n",
    "files = [f for f in files if not f.startswith('.')]\n",
    "\n",
    "\n",
    "sumkld_df = [] #initialize the df that will contain all the files' data\n",
    "\n",
    "for file in tqdm(files,position=0,colour='cornflowerblue'): \n",
    "    \n",
    "    kdedf = pd.read_csv(os.path.join(datapath,file))\n",
    "\n",
    "    neurons = list(set(kdedf['cell_id']))\n",
    "    \n",
    "    for neuron in neurons: \n",
    "        \n",
    "        cell_id = neuron\n",
    "        subneuron = kdedf[kdedf['cell_id']==cell_id]\n",
    "\n",
    "        conditions = list(set(subneuron['condition_labels']))\n",
    "\n",
    "        for condition in conditions: \n",
    "\n",
    "            subcond = subneuron[subneuron['condition_labels']==condition]\n",
    "\n",
    "            for rhythm in rhythms: \n",
    "\n",
    "                subrh = subcond[subcond['rhythm']==rhythm]\n",
    "\n",
    "                if (file.startswith('LH02')) | (file.startswith('LH03')): \n",
    "\n",
    "                    windows = ['one','two','three','four','five','six']\n",
    "\n",
    "                elif (file.startswith('LH08')) | (file.startswith('LH09')) | (file.startswith('LH16')): \n",
    "\n",
    "                    windows = ['one','two','three','four','five']\n",
    "\n",
    "                for window in windows: \n",
    "\n",
    "                    subwin = subrh[subrh['fitting_window']==window]\n",
    "\n",
    "\n",
    "                    uni_pspk = np.repeat(np.mean(subwin['kde_spikeprob']),subwin.shape[0])\n",
    "\n",
    "\n",
    "                    sumkl = np.sum(kl(subwin['kde_spikeprob'].values,uni_pspk))\n",
    "                    if np.isnan(sumkl): \n",
    "                        sumkl = 0\n",
    "\n",
    "                    cell_id = list(set(subwin['cell_id'].values))[0]\n",
    "                    quarter = list(set(subwin['quarter_labels']))[0]\n",
    "\n",
    "                    sumkld = {'cell_id': cell_id,\n",
    "                              'rhythm': rhythm,\n",
    "                              'quarter_labels': [quarter],\n",
    "                              'condition_labels': [condition],\n",
    "                              'fitting_window': [window],\n",
    "                              'sumkld': [sumkl]\n",
    "                             }\n",
    "\n",
    "\n",
    "                    sumkld_df.append(pd.DataFrame(sumkld))\n",
    "\n",
    "sumkld_df = pd.concat(sumkld_df)\n",
    "                \n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ce77f",
   "metadata": {},
   "source": [
    "### Figure 3C (entrainment time courses)\n",
    "example cell entrainment (we show trajectories for odors 7 & 8 in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaefe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_id = 'LH16_D10_TETSPK33a'\n",
    "\n",
    "subcell = sumkld_df[sumkld_df['cell_id']==cell_id]\n",
    "conditions = list(set(subcell['condition_labels']))\n",
    "nconds = len(conditions)\n",
    "\n",
    "fig,ax = plt.subplots(nconds,1,figsize=(20,30))\n",
    "\n",
    "rhythms = ['theta','lowgamma','highgamma']\n",
    "\n",
    "for c,condition in enumerate(conditions): \n",
    "\n",
    "    subcond = subcell[subcell['condition_labels']==condition]\n",
    "    \n",
    "    if c==0: \n",
    "        \n",
    "        should_legend_be_on = 'auto'\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        should_legend_be_on = False\n",
    "        \n",
    "    theta_sumkls = subcond[subcond['rhythm']=='theta']['sumkld'].values\n",
    "    lowgamma_sumkls = subcond[subcond['rhythm']=='lowgamma']['sumkld'].values\n",
    "    highgamma_sumkls = subcond[subcond['rhythm']=='highgamma']['sumkld'].values\n",
    "\n",
    "    TL_pear_r, TL_pear_pval = stats.pearsonr(theta_sumkls,lowgamma_sumkls)\n",
    "    TH_pear_r, TH_pear_pval = stats.pearsonr(theta_sumkls,highgamma_sumkls)\n",
    "        \n",
    "    sns.lineplot(data=subcond.reset_index(),\n",
    "                 x = 'fitting_window',\n",
    "                 y = 'sumkld',\n",
    "                 hue= 'rhythm',\n",
    "                 ax=ax[c],\n",
    "                 legend=should_legend_be_on\n",
    "            ).set_title('odor '+str(condition) + '\\n TL pearcorr: ' + str(round(TL_pear_r,4))+ ' ' + str(round(TL_pear_pval,4)) + '\\n TH pearcorr: '+ str(round(TH_pear_r,4))+ ' ' + str(round(TH_pear_pval,4)))\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53074114",
   "metadata": {},
   "source": [
    "### Figure 3C (window phase models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = cell_id.split('TET')[0]+'kde_over_time.csv'\n",
    "\n",
    "kdedf = pd.read_csv(os.path.join(datapath,file))\n",
    "\n",
    "conditions = list(set(kdedf['condition_labels']))\n",
    "nconds = len(conditions)\n",
    "\n",
    "if file.startswith('LH02') | file.startswith('LH03'): \n",
    "        \n",
    "    windows = ['one','two','three','four','five','six']\n",
    "        \n",
    "else: \n",
    "        \n",
    "    windows = ['one','two','three','four','five']\n",
    "\n",
    "                   \n",
    "fig,ax = plt.subplots(nconds,len(windows),figsize=(20,round(len(windows)*3.7)))\n",
    "\n",
    "rhythms = ['theta','lowgamma','highgamma']\n",
    "\n",
    "subcell = kdedf[kdedf['cell_id']==cell_id]\n",
    "\n",
    "for c,condition in enumerate(conditions): \n",
    "\n",
    "    subcond = subcell[subcell['condition_labels']==condition]\n",
    "    \n",
    "    if c==0: \n",
    "        \n",
    "        should_legend_be_on = 'auto'\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        should_legend_be_on = False\n",
    "    \n",
    "    for w,window in enumerate(windows): \n",
    "        \n",
    "        subwin = subcond[subcond['fitting_window']==window]\n",
    "        \n",
    "        subtheta = subwin[subwin['rhythm']=='theta']\n",
    "        sublowgamma = subwin[subwin['rhythm']=='lowgamma']\n",
    "        subhighgamma = subwin[subwin['rhythm']=='highgamma']\n",
    "\n",
    "        sns.lineplot(data=subtheta,\n",
    "                     x = np.linspace(-np.pi,np.pi,1000),\n",
    "                     y = 'kde_spikeprob',\n",
    "                     color = 'blue',\n",
    "                     ax=ax[c][w],\n",
    "                     legend=should_legend_be_on\n",
    "                ) \n",
    "        \n",
    "        sns.lineplot(data=sublowgamma,\n",
    "                     x = np.linspace(-np.pi,np.pi,1000),\n",
    "                     y = 'kde_spikeprob',\n",
    "                     color = 'orange',\n",
    "                     ax=ax[c][w],\n",
    "                     legend=should_legend_be_on\n",
    "                ) \n",
    "        \n",
    "        sns.lineplot(data=subhighgamma,\n",
    "                     x = np.linspace(-np.pi,np.pi,1000),\n",
    "                     y = 'kde_spikeprob',\n",
    "                     color = 'darkgreen',\n",
    "                     ax=ax[c][w],\n",
    "                     legend=should_legend_be_on\n",
    "                ).set_ylabel('odor '+ str(condition)) \n",
    "        \n",
    "    for a in ax[c]: \n",
    "        \n",
    "        a.set_ylim(0,0.05)\n",
    "        \n",
    "    for a in ax[2]:\n",
    "        \n",
    "        a.set_ylim(0,0.05)\n",
    "        \n",
    "    for a in ax[3]:\n",
    "        \n",
    "        a.set_ylim(0,0.015)\n",
    "        \n",
    "fig.tight_layout()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece58d1",
   "metadata": {},
   "source": [
    "figure out which side (or sides) of the maze these two odors took place in for this rat. Odor ports 3 & 4 are on the right side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40caeccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ogpath = 'python_spkphase_odorsamp/'\n",
    "file = cell_id.split('TET')[0] + 'correct.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(ogpath,file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd182033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(df[df['odor_labels'] == 7]['pos_labels']))\n",
    "print(set(df[df['odor_labels'] == 8]['pos_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88c90a",
   "metadata": {},
   "source": [
    "### pearson correlations\n",
    "first, add cluster labels to each interneuron, so you can do the Pearson correlations only for spike trains that were in the entrained categories (rather than the insensitive categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df634f25",
   "metadata": {},
   "source": [
    "you would have created the loaded file in notebook `step2_visualize_clusters_odorsamp.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ffa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_id_df = pd.read_csv('cluster_ids_theta_lowgamma_highgamma.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b35f37",
   "metadata": {},
   "source": [
    "now merge the sum kld df with the cluster labels df!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumkld_clust_df = sumkld_df.merge(clust_id_df,on=['cell_id','condition_labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f163528",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumkld_clust_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95a8ad",
   "metadata": {},
   "source": [
    "#### gather up all the spike trains' correlations\n",
    "for theta-low gamma, and theta-high gamma sum kld correlations separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = list(set(sumkld_clust_df['cell_id']))\n",
    "rhythms = ['theta','lowgamma','highgamma']\n",
    "\n",
    "gather_peardfs = []\n",
    "for cell_id in cells: \n",
    "    \n",
    "    subcell = sumkld_clust_df[sumkld_clust_df['cell_id']==cell_id]\n",
    "    \n",
    "    conditions = list(set(subcell['condition_labels']))\n",
    "    nconds = len(conditions)\n",
    "\n",
    "    for c,condition in enumerate(conditions): \n",
    "\n",
    "        subcond = subcell[subcell['condition_labels']==condition]\n",
    "        \n",
    "        if (cell_id.startswith('LH02')) | (cell_id.startswith('LH03')): \n",
    "            \n",
    "            subcond = subcond[subcond['fitting_window'] != 'six']\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            subcond = subcond[subcond['fitting_window'] != 'nofit']\n",
    "\n",
    "        theta_sumkls = subcond[subcond['rhythm']=='theta']['sumkld'].values\n",
    "        lowgamma_sumkls = subcond[subcond['rhythm']=='lowgamma']['sumkld'].values\n",
    "        highgamma_sumkls = subcond[subcond['rhythm']=='highgamma']['sumkld'].values\n",
    "\n",
    "        TL_pear_r, TL_pear_pval = stats.pearsonr(theta_sumkls,lowgamma_sumkls)\n",
    "        TH_pear_r, TH_pear_pval = stats.pearsonr(theta_sumkls,highgamma_sumkls)\n",
    "        LH_pear_r, LH_pear_pval = stats.pearsonr(lowgamma_sumkls,highgamma_sumkls)\n",
    "        \n",
    "        theta_id = subcond['theta_clust_id'].iloc[0]\n",
    "        lowgamma_id = subcond['lowgamma_clust_id'].iloc[0]\n",
    "        highgamma_id = subcond['highgamma_clust_id'].iloc[0]\n",
    "        \n",
    "        d = {'cell_id': cell_id,\n",
    "             'condition_labels': [condition],\n",
    "             'TL_pearson_r': [TL_pear_r],\n",
    "             'TL_pearson_p': [TL_pear_pval],\n",
    "             'TH_pearson_r': [TH_pear_r],\n",
    "             'TH_pearson_p': [TH_pear_pval],\n",
    "             'LH_pearson_r': [LH_pear_r],\n",
    "             'LH_pearson_pval': [LH_pear_pval],\n",
    "             'theta_clust_id': [theta_id],\n",
    "             'lowgamma_clust_id': [lowgamma_id],\n",
    "             'highgamma_clust_id': [highgamma_id]\n",
    "            }\n",
    "        \n",
    "        gather_peardfs.append(pd.DataFrame(d))\n",
    "        \n",
    "pear_df = pd.concat(gather_peardfs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop LH03_D18_TETSPK49b, bc it's all zeros, there is no spiking at all -- this is the \n",
    "# neuron id that we drop from odor samp epochs, but that we keep for approach epoch analyses\n",
    "pear_df = pear_df[pear_df['cell_id'] != 'LH03_D18_TETSPK49b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828f4de",
   "metadata": {},
   "source": [
    "#### make histograms that drop any comparisons between rhythm-insensitive cells\n",
    "we don't want to just get pearson correlation coefficients between spike trains with no entrainment to the rhythms being considered--this is just way less meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19559264",
   "metadata": {},
   "outputs": [],
   "source": [
    "pear_df_strong_TL = pear_df[(pear_df['theta_clust_id'] != 3) & (pear_df['lowgamma_clust_id'] != 3)]\n",
    "pear_df_strong_TH = pear_df[(pear_df['theta_clust_id'] != 3) & (pear_df['highgamma_clust_id'] != 3)]\n",
    "pear_df_strong_LH = pear_df[(pear_df['lowgamma_clust_id'] != 3) & (pear_df['highgamma_clust_id'] != 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0941739",
   "metadata": {},
   "source": [
    "### Figure 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(10,3))\n",
    "\n",
    "binwidth = 0.2\n",
    "\n",
    "sns.histplot(pear_df_strong_TL['TL_pearson_r'],\n",
    "             color = 'red', \n",
    "             alpha=0.5,\n",
    "             binwidth=binwidth,\n",
    "             binrange=(-1,1),\n",
    "             ax=ax[0]).set_title('theta-low gamma \\n pearson r')\n",
    "\n",
    "sns.histplot(pear_df['TL_pearson_r'],\n",
    "             color = 'grey', \n",
    "             alpha=0.5,\n",
    "             binwidth=binwidth,\n",
    "             binrange=(-1,1),\n",
    "             ax=ax[0]).set_title('theta-low gamma \\n pearson r')\n",
    "\n",
    "sns.histplot(pear_df_strong_TH['TH_pearson_r'],\n",
    "             color = 'red', \n",
    "             alpha=0.5,\n",
    "             binwidth=binwidth,\n",
    "             binrange=(-1,1),\n",
    "             ax=ax[1]).set_title('theta-high gamma \\n pearson r')\n",
    "\n",
    "sns.histplot(pear_df['TH_pearson_r'],\n",
    "             color = 'grey', \n",
    "             alpha=0.5,\n",
    "             binwidth=binwidth,\n",
    "             binrange=(-1,1),\n",
    "             ax=ax[1]).set_title('theta-high gamma \\n pearson r')\n",
    "\n",
    "\n",
    "sns.histplot(pear_df_strong_LH['LH_pearson_r'],\n",
    "             color = 'red', \n",
    "             alpha=0.5,\n",
    "             binwidth=binwidth,\n",
    "             binrange=(-1,1),\n",
    "             ax=ax[2]).set_title('low gamma-high gamma \\n pearson r')\n",
    "\n",
    "sns.histplot(pear_df['LH_pearson_r'],\n",
    "             color = 'grey', \n",
    "             alpha=0.5,\n",
    "             binwidth=binwidth,\n",
    "             binrange=(-1,1),\n",
    "             ax=ax[2]).set_title('low gamma-high gamma \\n pearson r (grey all spktrns)')\n",
    "for a in ax: \n",
    "    \n",
    "    a.set_xlim(-1,1)\n",
    "    a.set_ylim(0,55)\n",
    "    \n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df8a99",
   "metadata": {},
   "source": [
    "#### now, count how many correlation types each cell experienced\n",
    "across their available spike trains. To do this, you'll first have to categorize each correlation into negatively correlated (less than -0.5, middling to no correlation, and greater than 0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcbf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_thresh = 0.5\n",
    "neg_thresh = -pos_thresh\n",
    "\n",
    "pear_df_strong_TL['TL_pearson_corrtype'] = ['poscorr' if (i > pos_thresh) else 'negcorr' if (i < neg_thresh) else 'nocorr' for i in pear_df_strong_TL['TL_pearson_r'].values]\n",
    "pear_df_strong_TH['TH_pearson_corrtype'] = ['poscorr' if (i > pos_thresh) else 'negcorr' if (i < neg_thresh) else 'nocorr' for i in pear_df_strong_TH['TH_pearson_r'].values]\n",
    "pear_df_strong_LH['LH_pearson_corrtype'] = ['poscorr' if (i > pos_thresh) else 'negcorr' if (i < neg_thresh) else 'nocorr' for i in pear_df_strong_LH['LH_pearson_r'].values]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f16cdc",
   "metadata": {},
   "source": [
    "##### theta-low gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pear_df_strong_TL['cell_id']),len(set(pear_df_strong_TL['cell_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell_id in cells: \n",
    "    \n",
    "    subcell = pear_df_strong_TL[pear_df_strong_TL['cell_id']==cell_id]\n",
    "    print('TL',cell_id, set(subcell['TL_pearson_corrtype']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f218129",
   "metadata": {},
   "source": [
    "##### theta-high gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pear_df_strong_TH['cell_id']),len(set(pear_df_strong_TH['cell_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c746b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell_id in cells: \n",
    "    \n",
    "    subcell = pear_df_strong_TH[pear_df_strong_TH['cell_id']==cell_id]\n",
    "    print('TH',cell_id, set(subcell['TH_pearson_corrtype']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea165da",
   "metadata": {},
   "source": [
    "##### low gamma-high gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pear_df_strong_LH['cell_id']),len(set(pear_df_strong_LH['cell_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85580087",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell_id in cells: \n",
    "    \n",
    "    subcell = pear_df_strong_LH[pear_df_strong_LH['cell_id']==cell_id]\n",
    "    print('LH',cell_id, set(subcell['LH_pearson_corrtype']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
